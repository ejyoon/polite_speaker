// time webppl infsocpres.wppl --require webppl-json --require ~/tools/webppl-sample-writer-fork 1
// requires webppl package: webppl-json

var index = last(process.argv) // load index as last command line index

var utterances = [
  "yes_terrible","yes_bad","yes_good","yes_amazing",
  "not_terrible","not_bad","not_good","not_amazing"
];

var negative_utterances = [
  "not_terrible","not_bad","not_good","not_amazing"
];

var states = [0, 1, 2, 3];

var statePrior = function(){
  return uniformDraw(states);
};

var foreach = function(fn, lst) {
  var foreach_ = function(i) {
    if (i < lst.length) {
      fn(lst[i]);
      foreach_(i + 1);
    }
  };
  foreach_(0);
};

var isNegation = function(utt){
  return (utt.split("_")[0] == "not")
};

var round = function(x){
  return Math.round(x * 100) / 100
}

var weightBins = map(round, _.range(0,1, 0.025))
var phiWeights = repeat(weightBins.length, function(){1})

var goalWeightPrior = Infer({model: function(){
  return uniformDraw(weightBins)
}})

var nBins = weightBins.length;
var kernelWidth = 3//nBins / 4;

var kernelFn = function(prevVal){
  var i = weightBins.indexOf(prevVal);
  var upper = (i + kernelWidth) > nBins ? nBins : i + kernelWidth;
  var lower = (i - kernelWidth) < 0 ? 0 : i - kernelWidth;
  return Categorical({vs: weightBins.slice(lower,upper),
                      ps: phiWeights.slice(lower,upper)})
}

// measured in Experiment 1
var literalSemantics = json.read('../../../02_analysis/01_data/literal_semantics.json');
var data = json.read('../../../02_analysis/01_data/speaker_production.json')

var litSemanticsPosteriorObj = _.fromPairs(map(function(u){
  return [u, _.fromPairs(map(function(s){
    var litParams = _.filter(literalSemantics, {state: s, utterance: u})[0]
    return [s, {a: litParams.posterior_b1, b: litParams.posterior_b2}]
  }, states))]
}, utterances))


var goals = _.uniq(_.map(data, "goal"));
var states = _.uniq(_.map(data, "true_state"));


var model = function(){

  var RSAparameters = {
    speakerOptimality: uniformDrift({a: 0, b: 20, width:2}),
    cost: uniformDrift({a: 1, b: 10, width:0.25})
  };

  var cost_yes = 1;
  var uttCosts = map(function(u) {
    return isNegation(u) ? Math.exp(-RSAparameters.cost) : Math.exp(-cost_yes)
  }, utterances)

  var utterancePrior = Infer({model: function(){
    return utterances[discrete(uttCosts)];
  }});

	var litSemantics = _.fromPairs(map(function(u){
	    return [u, _.fromPairs(map(function(s){
	      var litParams = litSemanticsPosteriorObj[u][s];
	      return [s, beta(litParams)]
	    }, states))]
	  }, utterances))

  var meaning = function(words, state){
    return flip(litSemantics[words][state]);
  };

  var listener0 = cache(function(utterance) {
    Infer({method: "enumerate"}, function(){
      var state = uniformDraw(states);
      var m = meaning(utterance, state);
      condition(m);
      return state;
    });
  }, 10000);

  var speaker1 = cache(function(state, phi) {
    Infer({method: "enumerate"}, function(){
      var utterance = sample(utterancePrior);

      var speakerOptimality = RSAparameters.speakerOptimality;
//       var alpha = RSAparameters.alpha;

      var L0 = listener0(utterance);

      var epistemicUtility = L0.score(state);
//       var socialUtility = expectation(L0, function(s){return alpha*s});
      var socialUtility = expectation(L0, function(s){return s});

      var eUtility = phi*epistemicUtility;
      var sUtility = (1-phi)*socialUtility;

      var speakerUtility = eUtility+sUtility;
//     	var speakerUtility = eUtility; //actual(1)

      factor(speakerOptimality*speakerUtility);

      return utterance;
    })
  }, 10000)

  var listener1 = cache(function(utterance) {
    Infer({method: "enumerate"}, function(){

      var phi = categorical ({vs: weightBins, ps: phiWeights})

      var state = uniformDraw(states);

      var S1 = speaker1(state, phi)
      observe(S1, utterance)

      return {
        state: state,
        phi: phi
      }
    })
  }, 10000)

  var speaker2 = cache(function(exptCondInfo) {
    Enumerate(function(){
      var state = exptCondInfo.state;

      var S1_phi = exptCondInfo.S1_phi;

      var selfpres_phi = exptCondInfo.selfpres_phi;
      var social_phi = exptCondInfo.social_phi;
      var informative_phi = exptCondInfo.informative_phi;

      var utterance = sample(utterancePrior);
//       var alpha = RSAparameters.alpha;

    var L1 = listener1(utterance)
	  var L1_goal = marginalize(L1, "phi");
	  var L1_state = marginalize(L1, "state");

    var epistemicUtility = L1_state.score(state);
    var selfpresUtility = L1_goal.score(S1_phi);
    var socialUtility = expectation(L1_state, function(s){return s});


    var speakerMixedUtility = informative_phi*epistemicUtility +
                              selfpres_phi*selfpresUtility +
                              social_phi*socialUtility


//     var selfEUtility = s2Goals.phi*selfEpiUtility;
//     var selfSUtility = (1-s2Goals.phi)*socialUtility;
// var speakerSelfUtility = selfEUtility+selfSUtility;

// var selfEpiUtility = L1.score({"state":state,"phi":s1Goals})


//    	factor(RSAparameters.speakerOptimality*speakerUtility); // actual(2)
//       factor(RSAparameters.speakerOptimality * L1.score({"state":state, "goals":s1Goals}))
// factor(RSAparameters.speakerOptimality*speakerSelfUtility); // self(2)
//     factor(RSAparameters.speakerOptimality * L1_state.score(state)); // self(3)
//    factor(RSAparameters.speakerOptimality * L1_goal.score(intendedGoals)); // self(4)
		factor(RSAparameters.speakerOptimality * speakerMixedUtility); // self(5)
	// factor(RSAparameters.speakerOptimality*speakerPartialMixedUtility); // self(ï¼–)

      return utterance

    })
  }, 10000)

	foreach(function(goal){

    var S1_phi = sample(goalWeightPrior, {driftKernel: kernelFn})

    var phi_selfPres = uniformDrift({a: 0, b: 1, width: 0.2})
    var phi_informative = uniformDrift({a: 0, b: 1, width: 0.2})
    var phi_social = uniformDrift({a: 0, b: 1, width: 0.2})
//     var social_phi = 1 - informative_phi;

    var normalized_phis = normalize([phi_selfPres, phi_informative, phi_social])
    var normalized_true_phis = normalize([phi_informative, phi_social])
// 	var normalized_phis = function() {
// 	var normalized_phis = normalize([phi_selfPres, phi_informative, phi_social])
// 	var normalized_true_phis = normalize([phi_informative, phi_social])
// 	 return [normalized_phis[0], normalized_true_phis[0], normalized_true_phis[1]]
// 	}

		foreach(function(state){

      var utteranceData = _.filter(data, {true_state: state, goal: goal});

      var exptConditionInfo = {
        state: state,
        utterance: false,
        S1_phi: S1_phi,
        selfpres_phi: normalized_phis[0],
        informative_phi: normalized_phis[1],
        social_phi: normalized_phis[2]
//         informative_phi: normalized_true_phis[0],
//         social_phi: normalized_true_phis[1]
      };

      var RSApredictions = speaker2(exptConditionInfo);

      mapData({data: utteranceData}, function(d){
        observe(RSApredictions, d.utterance)
      });

      var postSupport = RSApredictions.support();

      foreach(function(u){
				query.add(["posteriorPredictive", goal, state, u], Math.exp(RSApredictions.score(u)))
      }, postSupport)

    }, states)

		query.add(["phi_Selfpres", goal, "NA", "NA"], normalized_phis[0])
		query.add(["phi_informative", goal, "NA", "NA"], normalized_phis[1])
		query.add(["phi_social", goal, "NA", "NA"], normalized_phis[2])
		query.add(["phi_s1", goal, "NA", "NA"], S1_phi)

  }, goals)

	// display(_.values(litSemantics))

	foreach(function(utt){
		var statesObj = litSemantics[utt]
		foreach(function(state){
			query.add(["litSemantics", "NA", state, utt], statesObj[state])
		}, _.keys(statesObj))
	}, _.keys(litSemantics))


	query.add(["speakerOptimality", "NA", "NA", "NA"], RSAparameters.speakerOptimality)
	query.add(["cost", "NA", "NA", "NA"], RSAparameters.cost)

  return query

}

var header = "iteration,key,goal,state,utt,val"

// var totalIterations = 80000, lag =  4;
var totalIterations = 350000, lag =  15;
// var totalIterations = 50000, lag =  5;
var samples = totalIterations/lag, burn = totalIterations / 2;
var output_file = 'bda-s2_self5_allNorm_3heart_paramAdj-imh'+
 totalIterations + '_burn' + burn + '_lag' + lag + '_chain' + index + '.csv'

var callback = webpplSampleWriter.streamQueryCSV(output_file, header);

var output = Infer({
  model,
  samples: samples,
  burn: burn,
  lag: lag,
  // method: 'MCMC',
  method: 'incrementalMH',
  onlyMAP: true,
  verbose: T,
  verboseLag: totalIterations / 20,
  callbacks: [callback]
});

'output written to ' + output_file
