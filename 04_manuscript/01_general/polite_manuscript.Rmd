---
title             : "Polite speech emerges from competing social goals"
shorttitle        : "Modeling polite speech"

author: 
  - name          : "Erica J. Yoon"
    affiliation   : "1, *, †"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Serra Mall, Bldg. 420, Rm. 290, Stanford, CA 94305"
    email         : "ejyoon@stanford.edu"
  - name          : "Michael Henry Tessler"
    affiliation   : "1, †"
  - name          : "Noah D. Goodman"
    affiliation   : "1"
  - name          : "Michael C. Frank"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Psychology, Stanford University"
  - id            : "\\*"
    institution   : "Corresponding author"
  - id            : "\\†"
    institution   : "These authors contributed equally to this work."

author_note: |
  All authors designed research and wrote the paper; E.J.Y. and M.H.T. performed research and analyzed data. The authors declare no conflict of interest. This work was supported by NSERC PGS Doctoral scholarship PGSD3-454094-2014 to EJY, NSF Graduate Research Fellowship DGE-114747 to MHT, ONR grant N00014-13-1-0788 to NDG, and NSF grant BCS 1456077 to MCF.


abstract: |
  Language is a remarkably efficient tool for transmitting information. Yet human speakers make statements that are inefficient, imprecise, or even contrary to their own beliefs, all in the service of being polite. What rational machinery underlies polite language use? Here, we show that polite speech emerges from the competition of three communicative goals: to convey information, to be kind, and to present oneself in a good light. We formalize this goal tradeoff using a probabilistic model of utterance production, which predicts human utterance choices in socially-sensitive situations with high quantitative accuracy, and we show that our full model is superior to its variants with subsets of the three goals. This utility-theoretic approach to speech acts takes a step towards explaining the richness and subtlety of social language use.
  
keywords          : "politeness, computational modeling, communicative goals, pragmatics"
wordcount         : "3816"

header-includes:
  - \usepackage{xcolor}
  - \usepackage[skip=2pt,font=scriptsize]{caption}

bibliography      : ["politeness.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes
mask              : no

class             : "man"
output            : papaja::apa6_pdf
---

\definecolor{Blue}{RGB}{10,100,200}
\definecolor{Red}{RGB}{255,0,0}

\newcommand{\red}[1]{{\textcolor{Red}{#1}}}
\newcommand{\mht}[1]{{\textcolor{Blue}{[mht: #1]}}}



```{r include_packages, include = FALSE}
# check to see if user has packages. otherwise, install them...
list.of.packages <- c("tidyverse", "brms", "BayesFactor",
                      "jsonlite", "magrittr", "ggthemes",
                      "forcats", "here")
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)

if (!require("papaja")) devtools::install_github("crsh/papaja")
if (!require("rwebppl")) devtools::install_github("mhtess/rwebppl")

knitr::opts_chunk$set(echo=FALSE, warning=FALSE, cache=T, message=FALSE, sanitize = T, out.width = "\\textwidth", fig.pos = "!h")
library(papaja)
library(tidyverse)
library(rwebppl)
library(jsonlite)
library(magrittr)
library(ggthemes)
library(forcats)
library(langcog)
library(here)
library(lme4)
library(brms)
library(BayesFactor)

estimate_mode <- function(s) {
  d <- density(s)
  return(d$x[which.max(d$y)])
}
hdi_upper <- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","upper"])
}
hdi_lower <- function(s){
  m <- HPDinterval(mcmc(s))
  return(m["var1","lower"])
}
options("scipen"=10)   
```

```{r load_data}
# human data 
load(here("02_analysis/01_data/speaker_production.RData"))

# human data - negation
load(here("02_analysis/01_data/speaker_production_neg.RData"))

# model posterior predictives
load(here("03_model/01_posterior_predictives/03_output_processed/postpred_summary.RData"))

# model posterior predictives - negation
load(here("03_model/01_posterior_predictives/03_output_processed/postpred_neg_summary.RData"))

# put data and model predictions together
ms_utt <- rbind(ms_data, 
                ms_model) 

# put neg data and model predictions together
ms_neg <- rbind(ms_data_neg, 
                ms_model_neg) %>%
  mutate(goal = as.factor(goal))

```

# Introduction

We don’t always say what's on our minds.
Although “close the window!” could be sufficient, 
we dawdle, adding “can you please...?” or “would you mind...?” 
Rather than tell an uncomfortable truth, socially-aware speakers exaggerate (“Your dress looks great!”) and prevaricate (“Your poem was so appropriate to the occasion”).
Such language use is puzzling for classical views of language as information transfer [@buhler1934; @jakobson1960; @shannon1948; @frank2012]. 
On the classical view, transfer ought to be efficient and accurate: 
Speakers are expected to choose succinct utterances to convey their beliefs [@grice1975; @searle1975], 
and the information conveyed is ideally truthful to the extent of a speaker's knowledge. 
Polite speech violates these basic expectations about the nature of communication: 
It is typically inefficient and underinformative, and sometimes even outright false. 
Yet even young speakers spontaneously produce requests in polite forms [@axia1985], and adults use politeness strategies pervasively -- even while arguing [@holtgraves1997], and
even though polite utterances may risk high-stakes misunderstandings [@bonnefon2011risk].

If politeness only gets in the way of effective information transfer, why be polite?
Clearly there are social concerns, and most linguistic theories assume speaker behavior is motivated by these concerns, 
couched as either polite maxims [@leech1983], social norms [@ide1989], or aspects of a speaker and/or listener's identity, known as *face* [@goffman1967; @brown1987].
Face-based theories predict that when a speaker's intended meaning contains a threat to the listener's face or self-image (and potentially the speaker's face), her messages will be less direct, less efficient, and possibly untruthful.
Indeed, when interpreting utterances in face-threatening situations, listeners readily assume that speakers intend to be polite [@bonnefon2009].
How this socially-aware calculation unfolds, however, is not well understood.
Adopting an example from @bonnefon2009, when should a speaker decide to say something false ("Your poem was great!" said of an actually-mediocre poem) rather than to tell the truth (“Your poem was bad”) or to be indirect (“Some of the metaphors were tricky to understand.”)?
How do the speaker’s goals enter into the calculation?

We propose a utility-theoretic solution to the problem of understanding polite language, in which speakers choose their utterance by attempting to maximize utilities that represent competing communicative goals.
Under the classic pragmatic view of language production, speakers want to be informative and convey accurate information as efficiently as possible [@grice1975; @goodman2016]; this goal we call *informational utility*.
In addition, speakers may want to be kind and make the listener feel good (i.e., save the listener’s face), for example, by stating positive remarks about the listener.
We call this goal to be *prosocial utility*.

If a speaker wanted to be informative and kind, then she would ideally produce utterances that satisfy both goals. 
The nuances of reality, however, can make it difficult to satisfy both goals.
In particular, when what the true state of the world is of low value to the listener (e.g., the listener’s poem was terrible), informational and prosocial goals pull in opposite directions. 
Informational utility could be maximized by stating the blunt truth (“your poem was terrible.”) but that would very likely hurt the listener’s feelings and threaten the listener’s self-image (low prosocial utility); 
prosocial utility could be maximized through a white lie (“your poem was amazing”), but it would be misleading (low informational utility). 
In such situations, it seems impossible to be both truthful and kind. A first contribution of our work here is to formalize the details of this tradeoff so that it can make contact with experimental data. 

A second contribution of our work is to develop and test a new theoretical proposal, namely, that speakers may use indirect speech to present themselves positively when they are caught by the conflict between truthfulness and kindness. We propose that speakers may navigate their way out of this conflict by signalling to the listener that they care about both of the goals even though they are genuinely unable to fulfill them. We formalize this notion of *self-presentational utility* and show that it leads speakers to prefer more indirect speech, namely utterances that provide less information relative to their length than alternatives with a similar meaning. 

We look at indirect speech in this paper through negated adjectival phrases (e.g., “It wasn’t bad”).  
The relationship between negation and politeness is a topic of long-standing interest to linguists and psychologists [@stoffel190; @stern1937; @bolinger1972; @horn1989].
Comprehending negation, as a logical operation, can be psychologically more complex than comprehending an unnegated assertion; negating assertions can result in difficulty in processing [@clark; see @nordmeyer for an underlying pragmatic explanation] as well as failure to recognize or recall the asserted content [@macdonald1989; @lea2002]. 
Our interest in negation, however, is for its information-theoretic properties: Negating an assertion that has a specific meaning results in a meaning that is less precise and lower in informativity (e.g., negating “Alex has blue eyes” results in the statement that Alex has eyes that are some color other than blue”).
In our paradigm, we use negation as a way of turning a relatively direct statement (“It was terrible”) into an indirect statement (“It wasn’t terrible”) whose interpretation includes some possibilities that are consistent with the original (the poem was not terrible, but it was still pretty bad). 

Multifactorial, verbal theories -- like previous proposals regarding politeness -- are very difficult to relate directly to behavioral data. Therefore, to test our hypotheses about the factors underlying the production of polite language (what we refer to as its utility structure), we take a model comparison approach. We do this by formalizing the trade-off between different combinations of speakers' utilities in a class of probabilistic models of language use (the Rational Speech Act (RSA) framework; @frank2012; @goodman2016), with a particular focus on models with and without the self-presentational utility. In this framework, speakers are modeled as agents who choose utterances by reasoning about their potential effects on a listener, while listeners infer the meaning of an utterance by reasoning about speakers and what goals could have led them to produce their utterances.
These models build on the idea that human social cognition can
be approximated via reasoning about others as rational agents who act to
maximize their subjective utility [@baker2009action], a hypothesis which has found support in a wide variety of work with both adults and children [e.g., @jara2016naive; @liu2017ten].
Indeed, this class of pragmatic language models has been productively applied to understand a wide variety of complex linguistic behaviors, including vagueness [@lassiter2017adjectival], hyperbole [@kao2014], and irony [@kao2015], among others. 

```{r model, fig.pos = "h", fig.cap="Diagram of the model. Top: First-order polite speaker ($S_1$) produces an utterance by thinking about: (1) the true state of the world (i.e., how good a given performance was); (2) the reasoning of literal listener who updates his beliefs about the true state via the literal meanings of utterances (e.g., “not terrible” means approximately 1.5 heart out of 3 hearts) and their affective consequences for the listener; and (3) her goal of balancing informational and social utilities. Bottom: Second-order polite speaker ($S_2$) produces an utterance by thinking about (1) the true state; (2) the pragmatic listener who updates his beliefs about the true state and the first-order speaker $S_1$’s goal (via reasoning about the $S_1$ model); and (3) her goal of balancing informational, prosocial, and self-presentational utilities. Different utterances shown correspond to different weightings of the utility components."}
knitr::include_graphics("fig/model.png", dpi = 160)
```

# Model

RSA models are defined recursively such that speakers \(S\) reason about
listeners \(L\), and vice versa. We use a standard convention in indexing and say a pragmatic listener \(L_1\) reasons about what intended meaning
and goals would have led a speaker \(S_1\) to produce a particular
utterance. \(S_1\) reasons about a *literal listener*
\(L_0\), who is modeled as attending only to the literal meanings of words
(rather than their pragmatic implications), and hence grounds the
recursion (Figure \@ref(fig:model), top).
The target of our current work is a model of a polite speaker \(S_2\)
who reasons about what to say to \(L_1\) by
considering some combination of informational, social, and self-presentational goals (Figure \@ref(fig:model), bottom).

We evaluate our model's ability to predict human utterance choices
in situations where polite language use is expected.
Imagine Bob recited a poem and asked Ann how good it was.
Ann (\(S_2\)) produces an utterance \(w\) based on the true state of the world \(s\) (i.e., the
rating, in her mind, truly deserved by Bob's poem) and a set of goal weights
\(\hat{\phi}\), that determines how much Ann prioritizes each of the three possible goals.
Ann's production decision is softmax, which interpolates between
maximizing and probability matching [via $\lambda_{S_2}$; @goodman2013]:

$$P_{S_2}(w | s, \hat{\phi}) \propto \exp(\lambda_{S_2} \cdot \mathop{\mathbb{E}}[U_{total}(w; s; \hat{\phi}; \phi_{S_1})]).$$

We posit that a speaker's utility contains three distinct components: informational, social, and presentational. The
total utility $U_{total}$ of an utterance is thus the weighted combination of the three utilities minus the utterance cost \(C(w)\):

$$U_{total}(w; s; \hat{\phi}; \phi_{S_1}) = \phi_{inf} \cdot U_{inf}(w; s) + \phi_{soc} \cdot U_{soc}(w) + \phi_{pres} \cdot U_{pres}(w; \phi_{S_1}) - C(w).$$

We define *social utility* (\(U_{soc}\)) as the expected subjective utility of the state \(V(s)\) implied to the
pragmatic listener by the utterance: \(U_{soc}(w) = \mathbb{E}_{P_{L_1}(s \mid w)}[V(s)]\).
The subjective utility function \(V(s)\) could vary by culture and context; we test our model when states are explicit ratings (e.g., on a 4-point scale) and we assume a positive linear value relationship between states and values \(V\) to model a listener's preference to be in a highly rated state (e.g., Bob would prefer to have written a poem deserving 4 points rather than 1 point).

At the same time, a speaker may desire to be epistemically helpful, modeled as standard *informational utility* (\(U_{inf}\)).
The informational utility indexes the utterance's *surprisal*, or amount of information the listener (\(L_1\)) would still not know about the state of the world \(s\) after hearing the speaker's utterance \(w\) (e.g., how likely is Bob to guess Ann's actual opinion of the poem): \(U_{inf}(w) = \ln(P_{L_1}(s | w))\).
Speakers who optimize for informational utility produce accurate and informative utterances while those who optimize for social utility produce utterances that make the listener feel good.

If a listener is uncertain how their particular speaker is weighing the competing goals to be honest vs. kind (informational vs. social utilities), he might try to infer the weighting (e.g., "was she just being nice?").
But a sophisticated speaker can produce utterances in order to appear *as if* she had certain goals in mind, for example making the listener think that the speaker was being both kind and informative ("she wanted me to know the truth but without hurting my feelings").
The extent to which the speaker *appears* to the listener to have a particular goal in mind (e.g., to be kind) is the utterance's *presentational utility* (\(U_{pres}\)).
The speaker gains presentational utility when her listener believes she
has particular goals, represented by a mixture weighting \(\phi_{S_1}\) between trying to be genuinely informative vs. kind.
Formally,

$$U_{pres}(w; \phi_{S_1}) = \ln(P_{L_1}(\phi_{S_1} \mid w)) = \ln \int_s P_{L_1}(s, \phi_{S_1} \mid w).$$

\noindent The speaker conveys a particular weighting of informational
vs. social goals (\(\phi_{S_1}\)) by considering the
beliefs of listener \(L_1\), who hears an utterance and jointly infers
the speaker's utilities and the true state of the world:

$$P_{L_1}(s, \phi_{S_1} | w) \propto P_{S_1}(w | s, \phi_{S_1}) \cdot P(s) \cdot P(\phi_{S_1}).$$

\noindent The presentational utility is the highest-order term of the model, defined only for a speaker thinking about a listener who evaluates a speaker
(i.e., defined for \(S_2\), but not \(S_1\)).
Only the social and informational utilities are defined for the \(S_1\) speaker (via reasoning about \(L_0\)); thus, \(S_1\)'s utility weightings can be represented by a single number, the mixture parameter \(\phi_{S_1}\).
Definitions for \(S_1\) and  \(L_0\) otherwise mirror those of  \(S_2\) and  \(L_1\) and can be found in the Supplmentary Materials: Model details section.

Finally, more complex utterances incur a greater cost, \(C(w)\) --
capturing the general pressure towards economy in speech. In our work,
utterances with negation (e.g., *not terrible*) are assumed to
be slightly costlier than their equivalents with no negation (this cost is inferred
from data; see Supplementary Materials).

Within our experimental domain, we assume there are four possible states
of the world corresponding to the value placed on a particular referent
(e.g., the poem the speaker is commenting on), represented in terms of numbers of hearts (Figure \@ref(fig:model)):
\(S = {s_0,...,s_3}\). Since the rating scale is relatively abstract, we assume a uniform prior distribution
over possible states of the world. The set of utterances is \{*terrible*, *bad*, *good*, *amazing*, *not terrible*, *not bad*, *not good*, and *not amazing*\}. We
implemented this model using the probabilistic programming language
WebPPL [@dippl] and a demo can be found at \url{http://forestdb.org/models/politeness.html}.

# Model predictions

The pragmatic listener model \(L_1\) draws complex inferences about both the true state of the world (Fig. \ref{fig:L1inferences}A) and the speaker's goals (Figure \@ref(fig:L1inferences)B).
Upon hearing \emph{{[}Your poem{]} was terrible} (Figure \@ref(fig:L1inferences)A and \@ref(fig:L1inferences)B top-left), the listener infers the poem is probably truly terrible (i.e., worthy of zero hearts) and that the speaker has strong informational goals.
*It was amazing* is more ambiguous (Figure \@ref(fig:L1inferences)A and \@ref(fig:L1inferences)B top-right): The poem could indeed be worthy of three hearts, but it is also plausible the speaker had strong social goals and the poem was mediocre. 
Negation makes the meanings less precise and introduces more uncertainty into the inference about the state: A listener who hears *It wasn't amazing* sees it as a relatively kind way of saying that the poem was quite bad (0 or 1 hearts), inferring a balance of social and informational goals for the speaker (Figure \@ref(fig:L1inferences)A and \@ref(fig:L1inferences)B bottom-right).
*It wasn't terrible* is the most open-ended, leaving open the possibility that the poem was worthy of 0 hearts (i.e., *it was terrible*) but conveying to the listener that the speaker cares about both informational and social goals, with a slight preference of towards being social (Figure \@ref(fig:L1inferences)A and \@ref(fig:L1inferences)B bottom-left). 

The self-presentational utility guides the speaker \(S_2\) to care about how she will be viewed in the eyes of the listener \(L_1\) (Figure \@ref(fig:L1inferences)C).
If the speaker wants to present herself as someone who is socially-minded (e.g., informational mixture or $\phi_{S_1}$ of 0.05), she should produce direct, positive utterances (e.g., *amazing*).
The best way to appear honest (e.g., informational mixture of 0.95) is to say direct, negative utterances (e.g., *terrible*).
The desire to appear as someone concerned with telling the truth while also caring about the listener's feelings (e.g., $\phi_{S_1}$ of 0.25) leads the speaker to produce indirect utterances (e.g., *not terrible*).
Such indirect speech acts are sufficiently open-ended to include the
possibility that the poem was good, but the avoidance of a more direct utterance (e.g., *good*) provides the listener with a way to recover the true state (e.g., the poem was mediocre) by way of reasoning that the speaker cares about his feelings by not saying the blunt truth.

```{r schematicPredictions}
load(here("02_analysis/01_data/s2_schematic_predictions_v2.RData"))

fig.marginal.utility.s2 <- rs.s2 %>%
  mutate(utility = case_when(
    phi_informative > 0.9 ~ "informational",
    phi_social > 0.9 ~ "social",
    phi_selfpres > 0.9 & phi_S1 == 0.25 ~ "presentational \n(both-goal)",
    phi_selfpres > 0.9 & phi_S1 == 0.05 ~ "presentational \n(social)",
    phi_selfpres > 0.9 & phi_S1 == 0.95 ~ "presentational \n(informative)",
    phi_informative == phi_social & phi_selfpres < 0.1 ~ "inf, soc",
    phi_informative == phi_selfpres & phi_social < 0.1  & phi_S1 == 0.25~ "inf, pres",
    phi_social == phi_selfpres & phi_informative < 0.1  & phi_S1 == 0.25~ "soc, pres",
    phi_informative == phi_social & phi_social == phi_selfpres  & phi_S1 == 0.25~ "inf, soc, pres",
    TRUE ~ "NA"
  )) %>%
  mutate(utility = fct_relevel(utility, "informational", "social", "presentational \n(informative)", "presentational \n(social)", "presentational \n(both-goal)", "inf, soc", "inf, pres", "soc, pres", "inf, soc, pres")) %>%
  filter(state == 0, utility != "NA") %>%
  filter(utility %in% c("informational", "social", "presentational \n(informative)", "presentational \n(social)", "presentational \n(both-goal)")) %>%
  # filter(state == 0, phi_S1 %in% c(0.05, 0.25, 0.5, 0.75, 0.95), phi_selfpres > 0.9) %>%
  select(-phi_social, -phi_selfpres, -phi_informative, -state, -phi_S1) %>%
  gather(utt, prob, -utility) %>%
  separate(utt, into = c("positive", "utterance")) %>%
  mutate(positive = factor(positive, levels = c("yes", "not"),
                           labels = c("it was ~", "it wasn't ~")),
         utterance = factor(utterance, levels = c("terrible", "bad", "good", "amazing"))) %>%
  group_by(utility, positive, utterance) %>%
  summarize(prob = mean(prob)) %>%
  ggplot(., aes( x = utterance, color = positive, y = prob, group = positive))+
  geom_line()+
  facet_wrap(~utility, nrow = 1)+
  scale_color_ptol() +
  theme_few(base_size = 14)+
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        strip.text.y = element_text(angle = 180),
        legend.title = element_blank(),
        legend.position = "bottom",
        # title = element_text(size = 8),
        # axis.title.y = element_text(size = 11)
        )+
  # scale_y_continuous(limits = c(0, 0.75), breaks = c(0, 0.75), position = "left")+
  # ggtitle("Self-presentational mixture (0 = fully social; 1 = fully informational)")+
  ylab(expression("Schematic " * S[2] * "'s production probability"))

rs.s2.tidy <- rs.s2 %>%
  gather(utt, prob, -state, -phi_S1, -phi_informative, -phi_social, -phi_selfpres) %>% 
  mutate(phi_selfpres_bin = as.numeric(as.character(cut(phi_selfpres, 
                                                        breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), 
                                                        labels = c(0.1, 0.3, 0.5, 0.7, 0.9)))),
         phi_informative_bin = as.numeric(as.character(cut(phi_informative, 
                                                           breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), 
                                                           labels = c(0.1, 0.3, 0.5, 0.7, 0.9)))),
         phi_social_bin = as.numeric(as.character(cut(phi_social, 
                                                      breaks = c(0, 0.2, 0.4, 0.6, 0.8, 1), 
                                                      labels = c(0.1, 0.3, 0.5, 0.7, 0.9))))) %>%
  mutate( 
    goal = as.factor(case_when(
      phi_informative_bin == 0.9 & phi_social_bin == 0.1 & phi_selfpres_bin == 0.1 ~ "be truly informative",  
      phi_informative_bin == 0.1 & phi_social_bin == 0.9 & phi_selfpres_bin == 0.1 ~ "be truly social",
      # (phi_informative_bin == 0.5 | phi_informative_bin == 0.3 | phi_informative_bin == 0.7) & (phi_social_bin == 0.5 | phi_social_bin == 0.7 | phi_social_bin == 0.3) & phi_selfpres_bin == 0.1 ~ "be truly both",
      phi_informative_bin == 0.5 & phi_social_bin == 0.5 & phi_selfpres_bin == 0.1 & phi_S1 == 0.1  ~ "be truly both", 
      phi_informative_bin == 0.1 & phi_social_bin == 0.1 & (phi_selfpres_bin == 0.9 | phi_selfpres_bin == 0.7) & phi_S1 == 0.9  ~ "look informative", 
      phi_informative_bin == 0.1 & phi_social_bin == 0.1 & phi_selfpres_bin == 0.9 & phi_S1 == 0.1 ~ "look social", 
      phi_informative_bin == 0.1 & phi_social_bin == 0.1 & phi_selfpres_bin == 0.9 & (phi_S1 == 0.3) ~ "look both",
      TRUE ~ "NA"))) %>%
  mutate(goal = factor(goal, levels = c("be truly informative", "be truly social", "be truly both", "look informative","look social", "look both","NA")))

rs.s2.tidy2 <- rs.s2.tidy %>%
                   mutate(utt2 = utt) %>%
                   mutate(utt = fct_recode(utt, "not amazing" = "not_amazing", 
                                            "not terrible" = "not_terrible", 
                                            "not good" = "not_good", 
                                            "not bad" = "not_bad", 
                                            "amazing" = "yes_amazing", 
                                            "good" = "yes_good", 
                                            "bad" = "yes_bad", 
                                            "terrible" = "yes_terrible"),
                          utt = fct_relevel(utt, "terrible", "bad", "good", "amazing", "not terrible", "not bad", "not good", "not amazing")) %>%
                            
                   separate(utt2, c("positive", "word")) %>%
                   mutate(word = fct_relevel(word, "terrible", "bad", "good", "amazing")) %>%
                   mutate(positive = fct_relevel(positive, "yes")) %>%
                   mutate(positive = fct_recode(positive, "it was ~" = "yes", "it wasn't ~" = "not")) %>%
                   mutate(goal = case_when(phi_informative_bin == 0.9 ~ "informational", 
                                           phi_social_bin == 0.9 ~ "social",
                                           phi_selfpres_bin == 0.9 & phi_S1 == 0.05 ~ "presentational \n(social S1)",
                                           phi_selfpres_bin == 0.9 & phi_S1 == 0.25 ~ "presentational \n(both-goal S1)",
                                           phi_selfpres_bin == 0.9 & phi_S1 == 0.95 ~ "presentational \n(informative S1)")) %>%
                   mutate(goal = fct_relevel(goal, "informational", "social", "presentational \n(informative S1)", "presentational \n(social S1)", "presentational \n(both-goal S1)")) %>%
                   filter(!is.na(goal))
                   # filter(phi_informative_bin == 0.9)

fig.s2 <- ggplot(rs.s2.tidy2, 
                 aes ( x = state, y = utt, fill = prob))+
  geom_tile(colour = "grey95") + 
  scale_fill_gradient(low = "white", high = "orange")+
  facet_grid(positive ~ goal, switch="y", scales = "free_y")+
  guides(fill = F)+
  ylab("utterance") +
  xlab("true state") +
    theme_few(base_size = 14) +
  theme(
    # axis.text.x = element_text(angle = 45, hjust=1),
        # strip.text.x = element_text(angle = 180),
        strip.text.y = element_blank()
        )  
```


```{r comparison}

plot.comp <- ms_utt %>%  
  mutate(model = case_when(
    model == "NA" ~ "human data", 
    model == "inf" ~ "model: \ninformational \nonly", 
    model == "soc" ~ "model: \nsocial only", 
    model == "pres" ~ "model: \npresentational \nonly", 
    model == "inf_pres" ~ "model: \ninformational, \npresentational", 
    model == "inf_soc" ~ "model: \ninformational, \nsocial", 
    model == "soc_pres" ~ "model: \nsocial, \npresentational", 
    model == "full" ~ "model: \ninformational, \nsocial, \npresentational" 
  ),
  model = fct_relevel(model, "model: \ninformational \nonly", "model: \nsocial only", "model: \npresentational \nonly", "model: \nsocial, \npresentational", "model: \ninformational, \nsocial", "model: \ninformational, \npresentational", "model: \ninformational, \nsocial, \npresentational")) %>%
  mutate(positivity = fct_recode(positivity,
                                 "it was ~" = "yes",
                                 "it wasn't ~" = "not",
                                 "it was ~" = "It was~",
                                 "it wasn't ~" = "It wasn't~"
                                 ),
         positivity = fct_relevel(positivity, "it was ~")) %>%
  filter(goal == "both", true_state == "0") %>%
  ggplot(., 
       aes(x=utterance, y=prob, fill=positivity, 
           # group = interaction(positive, source), linetype = forcats::fct_rev(positive),
           group = positivity,
           colour = positivity)) +
  geom_hline(yintercept=.125, lty=2, color="gray") +
  geom_line()+
  facet_grid(.~model) +
  xlab("utterance") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position="dodge") +
  ylim(0,.7)+
  scale_color_ptol(guide=FALSE) +
  # scale_color_solarized(guide=FALSE)+
  ggthemes::theme_few(base_size = 13)+
  ylab("proportion chosen") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position="bottom") +
  guides(color=guide_legend(title=NULL))

# plot.comp

ggsave("model_comparisons.png", plot = plot.comp, width = 11, height = 4,
       path = here::here("02_analysis/03_figs"))

```



```{r schematicPredictionsFig, fig.cap="Schematic predictions. Comparison of predictions for proportion of utterances chosen by pragmatic speaker from possible model variants (left) and human data (rightmost) for average proportion of negation produced among all utterances, given true state of 0 heart (on a scale of 0 to 3) and speaker with both goals to be informative and kind. Gray dotted line indicates chance level at 12.5\\%."}
schematic <- 
  cowplot::plot_grid(
  # cowplot::plot_grid(
  #   fig.marginal.state.l1,
  #   fig.marginal.phi.l1,
  #   nrow = 1,
  #   rel_widths = c(1, 1.2),
  #   labels = c("A", "B")
  # ),
cowplot::plot_grid(
  fig.s2,
  nrow = 1, 
  labels = c("", "A")),  
cowplot::plot_grid(
  fig.marginal.utility.s2,
  nrow = 1, 
    rel_widths = c(0.2, 1.2),
  labels = c("", "B")), 
cowplot::plot_grid(
  plot.comp,
  nrow = 1, 
    rel_widths = c(0.2, 1.2),
  labels = c("", "C")), 
ncol=1
)

ggsave(schematic, file="../../04_manuscript/01_general/fig/L1_S2_schematic.png", width = 9, height = 11)
knitr::include_graphics("fig/L1_S2_schematic", dpi = 170)
```

# Experiment: Speaker production task

We made a direct, fully pre-registered test of our speaker production model and its performance in comparison to a range of alternative models, by instantiating our running example in an online experiment.

```{r screenshot, fig.pos = "!h", fig.cap="Example of a trial in the speaker production task."}
knitr::include_graphics("fig/screenshot.png", dpi = 130)
```


## Participants

202 participants with IP addresses in the United States were recruited on Amazon’s Mechanical Turk.

## Design and Methods

Participants read scenarios with information on the speaker's feelings toward some performance or product (e.g., a poem recital; *true state*), on a scale from zero to three hearts (e.g.,
one out of three hearts). For example, one trial read: *Imagine that Bob gave a poem recital, but he didn't know how good it was. Bob approached Ann, who knows a lot about poems, and asked* ``How was my poem?'' 
Additionally, we manipulated the speaker's goals across trials: to be *informative*  (“give accurate and informative feedback”); to be *kind* (“make the listener feel good”); or to be *both* informative and kind simultaneously. We hypothesized that
each of the three experimentally-induced goals would induce a different tradeoff between social and informational
utilities in our model, as well as modulating the self-presentational component. In a single
trial, each scenario was followed by a question asking for the most
likely produced utterance by Ann. Participants selected one of eight possible
utterances, by choosing between *It was* vs. *It wasn’t* and then among *terrible*, *bad*, *good*, and *amazing.* 

Each participant read twelve scenarios, depicting every possible combination of the three goals and four states. 
The order of context items was randomized, and there were a maximum of two repeats of each context item per participant.
Each scenario was followed by a question that read, "If Ann wanted to make Bob feel good but not necessarily give informative feedback (or to give accurate and informative feedback but not necessarily make Bob feel good, or BOTH make Bob feel good AND give accurate and informative feedback), what would Ann be most likely to say?" 
Participants indicated their answer by choosing one of the options on the two dropdown menus, side-by-side, one for choosing between *It was* vs. *It wasn’t* and the other for choosing among *terrible*, *bad*, *good*, and *amazing.*

## Behavioral results

```{r data_bayes_factor}
load(file=here("03_model/03_other_stat/bayesfactor_state_goal_interaction.Rds"))
load(file=here("03_model/03_other_stat/bayesfactor_state_goal_main_effects.Rds"))
load(file=here("03_model/03_other_stat/bayesfactor_state_only.Rds"))

BF_full_vs_main <- model1bf1/model2bf1
BF_full_vs_one <- model1bf1/model3bf1
BF_main_vs_one <- model2bf1/model3bf1
```

```{r variance, echo=FALSE, fig.cap="Full distribution of human responses vs. model predictions. Error bars represent 95\\% confidence intervals for the data (vertical) and 95\\% highest density intervals for the model (horizontal)."}
ms_var <- ms_utt %>%
  filter(source == "data" | model == "full") %>%
  select(-model) %>%
  gather(var, value, ci_lower:prob, -source, -true_state) %>%
  unite(new, c(source, var)) %>%
  mutate(positivity = fct_recode(positivity, 
                               "It was ~" = "yes",
                               "It wasn't ~" = "not",
                               "It was ~" = "It was~",
                               "It wasn't ~" = "It wasn't~"
                               ),
         positivity = fct_relevel(positivity, "It was ~"),
         goal = fct_recode(goal, "kind" = "social")) %>%
  spread(new, value)

plot_var <- ggplot(ms_var,
       aes(x = model_prob, y = data_prob)) +
  aes(shape = factor(positivity)) +
  geom_point(aes(colour = factor(goal), fill = factor(goal)), size = 2) +
  scale_shape(solid = FALSE) +
  scale_shape_manual(name = "utterance type", values = c(21, 24))+
  theme_few()+
  geom_abline(intercept = 0, slope = 1, linetype = 3) +
  geom_errorbar(aes(ymin=data_ci_lower,ymax=data_ci_upper), alpha = 0.3) +
  geom_errorbarh(aes(xmin=model_ci_lower,xmax=model_ci_upper), alpha = 0.3) +
  xlab("model posterior predictive") +
  ylab("human proportion responses") +
  ylim(0,1) +
  xlim(0,1) +
  scale_y_continuous(breaks=c(0, 0.25, 0.5, 0.75, 1)) +
  scale_x_continuous(breaks=c(0, .25, 0.5, 0.75, 1)) +
  theme(axis.text.y = element_text(hjust = 0, angle = 0),
        axis.text.x = element_text(vjust = 0, angle = 0),
        legend.position = "bottom",
        legend.direction = "horizontal") +
  coord_fixed()+
  scale_colour_solarized(name = "goal") +
  scale_fill_solarized() +
  guides(fill=FALSE)

plot_var

ggsave("speaker_production_cor.png", plot = plot_var, width = 7, height = 4,
       path = here::here("02_analysis/03_figs"))

cor2_mainMod = with(ms_var, cor(data_prob, model_prob))^2

```

```{r brmstat}
load(here("03_model/03_other_stat/brms_polite.Rds"))

brm.tab <- as.data.frame(summary(brms_pol)$fixed) %>%
  select(Estimate, Est.Error, 'l-95% CI', 'u-95% CI') %>%
  rename(Mean = Estimate,
         "SD" = Est.Error,
         "95% CI-Lower" = 'l-95% CI',
         "95% CI-Upper" = 'u-95% CI'
         )

brm.tab$Predictor <- c("Intercept",
                      "True state",
                      "Goal: Informative",
                      "Goal: Kind",
                      "True state * Informative",
                      "True state * Kind"
                      )
rownames(brm.tab) <- NULL
brm.tab <- brm.tab[,c(5,1:4)]
```

Our primary behavioral hypothesis was that speakers describing bad
states (e.g., poem deserving 0 hearts) with goals to be both
informative and kind would produce more indirect, negative utterances
(e.g., *It wasn't terrible*). Such indirect speech acts both
save the listener's face and provide some information about the
true state, and thus, are what a socially-conscious speaker would say (Figure \@ref(fig:L1inferences)).
This prediction was confirmed, as a Bayesian mixed-effects
model predicts more negation as a function of true state and goal via
an interaction: A speaker with both goals to be informative and
kind produced more negation in worse states compared to a speaker with
only the goal to be informative (*M* = `r brm.tab$Mean[5]`, [`r brm.tab$"95% CI-Lower"[5]`, `r brm.tab$"95% CI-Upper"[5]`]) and goal to be kind (*M* = `r brm.tab$Mean[6]`, [`r brm.tab$"95% CI-Lower"[6]`, `r brm.tab$"95% CI-Upper"[6]`]). Rather than
eschewing one of their goals to increase utility along a single
dimension, participants chose utterances that jointly satisfied their
conflicting goals by producing indirect speech.


```{r comparisonVar}
ms_var_comp <- ms_utt %>%
  select(-ci_lower, -ci_upper) %>%
  mutate(model = case_when(
    model == "NA" ~ "data", 
    TRUE ~ as.character(model)
  )) %>%
  mutate(positivity = fct_recode(positivity, 
                               "It was ~" = "yes",
                               "It wasn't ~" = "not",
                               "It was ~" = "It was~",
                               "It wasn't ~" = "It wasn't~"
                               ),
         positivity = fct_relevel(positivity, "It was ~")) %>%
  select(-source) %>%
  spread(model, prob)

cor2_inf = with(ms_var_comp, cor(data, inf))^2
cor2_soc = with(ms_var_comp, cor(data, soc))^2
cor2_pres = with(ms_var_comp, cor(data, pres))^2
cor2_socpres = with(ms_var_comp, cor(data, soc_pres))^2
cor2_infsoc = with(ms_var_comp, cor(data, inf_soc))^2
cor2_infpres = with(ms_var_comp, cor(data, inf_pres))^2
cor2_infsocpres = with(ms_var_comp, cor(data, full))^2

```

```{r comparisonML}
ml_infsocpres <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_self5_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]
ml_infpres <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_self6_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]
ml_infsoc <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_actual2_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]
ml_socpres <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_self7_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]
ml_inf <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_trueInf_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]
ml_soc <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_trueSoc_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]
ml_pres <- fromJSON(here("03_model/02_model_comparison/02_output/ais-s2_selfPres_3heart_step150000_sample4_chain3.json"), flatten = TRUE, simplifyDataFrame = TRUE)[,1]

bf_full_vs_infpres <- mean(ml_infpres) - mean(ml_infsocpres)
bf_full_vs_infsoc <- mean(ml_infsoc) - mean(ml_infsocpres)
bf_full_vs_socpres <- mean(ml_socpres) - mean(ml_infsocpres)
bf_full_vs_inf <- mean(ml_inf) - mean(ml_infsocpres)
bf_full_vs_soc <- mean(ml_soc) - mean(ml_infsocpres)
bf_full_vs_pres <- mean(ml_pres) - mean(ml_infsocpres)

bf_list <- list(bf_full_vs_inf, bf_full_vs_soc, bf_full_vs_pres,  bf_full_vs_socpres, bf_full_vs_infsoc, bf_full_vs_infpres)
bf_list_rounded <- rapply(bf_list, function(x) round(x, digits=2))
bf_list_rounded_rev <- rev(append(bf_list_rounded, "--"))
```

```{r comparisonTable, results = 'asis'}
model_list = rev(c("informational \nonly", "social only", "presentational \nonly", "social, \npresentational", "informational, \nsocial", "informational, \npresentational", "informational, \nsocial, \npresentational"))
variance_list = rev(c(cor2_inf, cor2_soc, cor2_pres, cor2_socpres, 
                           cor2_infsoc, cor2_infpres, cor2_infsocpres))

comp_tab <- data.frame("model" = model_list, 
                       "variance" = variance_list, 
                       "logBF" = bf_list_rounded_rev) %>%
  rename("variance \nexplained" = "variance",
         "log BF" = "logBF")

apa_table(comp_tab, caption = "Comparison of variance explained for each model variant and log Bayes Factors quantifying evidence in favor of alternative model in comparison.")
```

## Model results

The model parameters (softmax parameters and each goal condition's utility weights) can be inferred from the behavioral data using Bayesian data analysis [@lee2014].
To approximate the literal meanings (i.e., the semantics) of the words as interpreted by the literal listener \(L_0\), we obtained literal meaning judgments from an independent group of participants
(See Supplmentary Materials: Literal semantic task section).
The posterior predictions from the the three-utility polite speaker model (informational, social, presentational) showed a very strong fit to participants' actual utterance choices ($r^2$(96) = `r cor2_mainMod`; Figure \@ref(fig:variance)).
We compared these to six model variants containing
subsets of the three utilities in the full model.
Both the variance explained and marginal likelihood of the observed data were the
highest for the full model (Table \@ref(tab:comparisonTable)). Only the
full model captured participants' preference for negation when the speaker wanted to be informative and kind
about truly bad states, as hypothesized (Figure \@ref(fig:comparison)).
In sum, the full set of informational, social, and presentational were
required to fully explain participants' utterance choices.

```{r phi, results = 'asis'}
load(here("03_model/01_posterior_predictives/03_output_processed/phi_summary.RData"))
load(here("03_model/01_posterior_predictives/03_output_processed/other_param_summary.RData"))

phi_tab <- d_phi_s %>%
  mutate_at(vars(informative:social), as.numeric) %>%
  mutate_if(is.numeric, function(x) round(x, digits=2)) %>%
  mutate_at(vars(informative:social), 
            funs(dplyr::case_when(is.na(.) ~ "--",
                                  TRUE ~ as.character(.)))) %>%
  # mutate_if(is.numeric, function(x) ifelse(is.na(x), "--", x)) %>%
  ungroup() %>%
    mutate(model = case_when(
    model == "inf" ~ "informational only", 
    model == "soc" ~ "social only", 
    model == "pres" ~ "presentational only", 
    model == "InfPres" ~ "informational, presentational", 
    model == "InfSoc" ~ "informational, social", 
    model == "SocPres" ~ "social, presentational", 
    model == "Full" ~ "informational, social, presentational" 
  )) %>%
  select(model, goal, informative, social, Selfpres, s1)

colnames(phi_tab) <- c("model (utilities)", "goal", "$\\phi_{inf}$", "$\\phi_{soc}$", "$\\phi_{pres}$", "$\\phi_{S_1}$")

apa_table(phi_tab, escape=FALSE, caption = "Inferred phi parameters from all model variants with more than one utility.")
```

The utility weights inferred for the three-utility model (Table \@ref(tab:phi))
provide additional insight into how polite language use operates in our experimental context and possibly beyond:
*Being kind* ("social") requires not only weights on social and presentational utilities but equal weights on all three utilities,
indicating that informativity is a part of language use
even when it is explicitly not the goal.
*Being informative* ("informative") pushes
the weight on social utility ($\phi_{soc}$) close to zero, but the weight on 
*appearing kind* ($\phi_{pres}$) stays high, suggesting that speakers are expected
to manage their own face even when they are not considering others'.
*Kind and informative* ("both") speakers emphasize informativity slightly
more than kindness. 
In all cases, however, the presentational utilities have greatest weight, 
suggesting that managing the listener's inferences about oneself was integral to participants' decisions in the context of our  communicative task.
Overall then, our condition manipulation altered the balance between these weights, 
but all utilities played a role in all conditions.

# Discussion

Politeness is puzzling from an information-theoretic perspective.
Incorporating social motivations adds a level of explanation, but so far such intuitions and observations have resisted both formalization and precise testing.
We present a utility-theoretic model of language use that captures the interplay between competing
informational, social, and presentational goals, and provide preregistered
experimental evidence that confirmed its ability to capture human
judgments, unlike comparison models with only a subset of the full
utility structure.

To estimate precisely choice behavior in the experiment,
it was required to abstract away
from natural interactions in a number of ways.
Human speakers have access
to a potentially infinite set of utterances to select from in order to manage the three-utility tradeoff (*It's hard to write a good poem*, *That metaphor in the second stanza was so relatable!*). In theory,
each utterance will have strengths and weaknesses relative to the
speaker's goals, though computation in an unbounded model presents
technical challenges [perhaps paralleling the difficulty human speakers
feel in finding the right thing to say in a difficult situation; see @goodman2016].

For a socially-conscious speaker, managing listeners' inferences is a
fundamental task. Our work extends previous models of language beyond
standard informational utilities to address social and
self-presentational concerns. Further, our model builds upon the theory of politeness as face
management [@brown1987] and takes a step towards
understanding the complex set of social concerns involved in face management.
Our approach can provide insight into a wide range of social behaviors beyond speech by considering utility-driven inferences in a
social context [@baker2017rational; @hamlin2013mentalistic] where agents need to take
into account concerns about both self and others.

Previous game-theoretic analyses of politeness have either required some social cost to an utterance [e.g., by reducing one's social status or incurring social debt to one's conversational partner; @vanRooy2003] or a separately-motivated notion of plausible deniability [@pinker2008].
The kind of utterance cost for the first type of account would necessarily involve higher-order reasoning about other agents, and may be able to be defined in terms of the more basic social and self-presentational goals we formalize here.
A separate notion of plausible deniability may not be needed to explain most politeness behavior, either.
Maintaining plausible deniability is in one's own self-interest (e.g., due to controversial viewpoints or covert deception) and goes against the interests of the addressee; some amount of utility dis-alignment is presumed by these accounts. Politeness behavior appears present even in the absence of obvious conflict, however: In fact, you might be even more motivated to be polite to someone whose utilities are more aligned with yours (e.g., a friend). In our work here, we show that such behaviors can in fact arise from purely cooperative goals [@brown1987], though in cases of genuine conflict, plausible deniability likely plays a more central role in communication.

Utility weights and value functions in our model could provide a framework for a quantitative understanding of systematic
cross-cultural differences in what counts as polite. Cross-cultural differences in politeness could be a product of different weightings within the same
utility structure. Alternatively, culture could affect
the value function \(V\) that maps states of the world onto subjective
values for the listener (e.g., the mapping from states to utilities may
be nonlinear and involve reasoning about the future).
Our formal modeling approach with systematic behavior measurements provides an avenue towards understanding the vast range of politeness practices found across languages.


Politeness is only one of the ways language use deviates from purely
informational transmission.
We flirt, insult, boast, and empathize by balancing informative transmissions with goals to affect others' feelings or
present particular views of ourselves.
Our work shows how social and
self-presentational motives are integrated with informational concerns more
generally, opening up the possibility for a broader theory of social
language.
In addition, a formal account of politeness moves us closer to
courteous computation -- to machines that can talk with tact.

\newpage

# Supplementary Materials

## Model details

The *literal listener* $L_0$ is a simple Bayesian agent that takes the utterance to be true:

$$P_{L_0}(s | w) \propto [\![ w ]\!] (s) * P(s).$$

\noindent where $[\![ w ]\!](s)$ is the truth-functional denotation of the utterance
$w$ (i.e. the utterance's literal meaning): It is a function
that maps world-states $s$ to Boolean truth values. The literal
meaning is used to update the literal listener's prior beliefs
over world states $P(s)$.

The *speaker* $S_1$ chooses utterances approximately optimally given a utility function, which can be decomposed into two components. 
First, informational utility ($U_{inf}$) is the amount of information a literal listener $L_0$ would still not know about world state $s$ after hearing a speaker's utterance $w$. 
Second, social utility ($U_{soc}$) is the expected subjective utility of the state inferred given the utterance $w$. 
The utility of an utterance subtracts the cost $c(w)$ from the weighted combination of the social and epistemic utilities. 

$$U(w; s; \phi_{S_1}) = \phi_{S_1} \cdot \ln(P_{L_0}(s \mid w)) + (1 - \phi_{S_1}) \cdot \mathbb{E}_{P_{L_0}(s \mid w)}[V(s)] - C(w).$$

\noindent The speaker then chooses utterances $w$ softmax-optimally given the state $s$ and his goal weight mixture $\phi_{S_1}$: 

$$P_{S_1}(w \mid s, \phi_{S_1}) \propto \mathrm{exp}(\lambda_{1} \cdot \mathbb{E}[U(w; s; \phi_{S_1})]).$$

## Literal semantic task

We probed judgments of literal meanings of the target words assumed by our model and used in our main experiment. 

### Participants 

51 participants with IP addresses in the United States were recruited on Amazon’s Mechanical Turk. 

### Design and Methods

We used thirteen different context items in which a speaker evaluated a performance of some kind. 
For example, in one of the contexts, Ann saw a presentation, and Ann’s feelings toward the presentation (true state) were shown on a scale from zero to three hearts (e.g., two out of three hearts filled in red color; see Figure\ \@ref(fig:screenshot) for an example of the heart scale). 
The question of interest was "Do you think Ann thought the presentation was / wasn’t X?" and participants responded by choosing either “no” or “yes.” 
The target could be one of four possible words: *terrible*, *bad*, *good*, and *amazing*, giving rise to eight different possible utterances (with negation or no negation). 
Each participant read 32 scenarios, depicting every possible combination of states and utterances. 
The order of context items was randomized, and there were a maximum of four repeats of each context item per participant. 

### Behavioral results

We analyzed the data by collapsing across context items.
For each utterance-state pair, we computed the posterior distribution over the semantic weight (i.e., how consistent X utterance is with Y state) assuming a uniform prior over the weight (i.e., a standard Beta-Binomial model). 
Meanings of the words as judged by participants were as one would expect (Figure \ \@ref(fig:litsem)). 

```{r litsem, echo=FALSE, fig.width = 10, fig.height = 4, out.width = "\\textwidth", fig.pos = "!h", fig.cap = "Semantic measurement results. Proportion of acceptances of utterance types (shown in different colors) combined with target words (shown in different facets) given the true state represented on a scale of hearts. Error bars represent 95\\% confidence intervals."}
d <- read.csv(here("02_analysis/01_data/literal_semantics.csv")) %>%
  mutate(utterance = fct_relevel(utterance, "terrible", "bad", "good", "amazing"))

ms <- d %>%
  mutate(positivity = fct_recode(positivity,
                                "it was ~ " = "it was ___",
                                "it wasn't ~ " = "it wasn't ___"
                                ),
         positivity = fct_relevel(positivity, "it was ~ ")) %>%
  group_by(positivity, state, utterance, subid) %>%
  summarize(
            judgment = mean(judgment, na.rm=TRUE)
          ) %>%
  group_by(positivity, state, utterance) %>%
  multi_boot_standard(col = "judgment") %>%
  mutate(judgment = mean)

qplot(state, judgment, 
      colour = positivity,
      data=ms) + 
  geom_line(aes(group=positivity)) +
  facet_grid(.~utterance) +
  xlab("state (0=worst)") +
  ylab("proportion of\n acceptances") +
  geom_errorbar(aes(ymin=ci_lower,ymax=ci_upper,width=.1)) +
  theme_few(base_size = 16) +
  scale_color_ptol(name="")+
  # scale_color_solarized() +
  theme(legend.position = "bottom") +
  geom_hline(yintercept = .5, lty=2)

ggsave("literal_semantics.png", width = 7, height = 3,
       path = here::here("02_analysis/03_figs"))
```

## Data analysis

We used `r cite_r("politeness.bib")` for all our analyses.

## Full statistics on human data

```{r brmTab, results="asis"}
apa_table(brm.tab, caption= "Predictor mean estimates with standard deviation and 95% credible interval information for a Bayesian linear mixed-effects model predicting negation production based on true state and speaker goal (with both-goal as the reference level).")
```

We used Bayesian linear mixed-effects models [`brms` package in R; @R-brms] using crossed random effects of true state and goal with maximal random effects structure [@gelman2006data; @barr2013random]. The full statistics are shown in Table \@ref(tab:brmTab).

## Model fitting and inferred parameters

```{r otherParams, results='asis'}
other_tab <- d_other_s %>%
    mutate(model = case_when(
    model == "inf" ~ "ninformational only", 
    model == "soc" ~ "social only", 
    model == "pres" ~ "presentational only", 
    model == "inf_pres" ~ "ninformational, presentational", 
    model == "inf_soc" ~ "ninformational, social", 
    model == "soc_pres" ~ "social, presentational", 
    model == "full" ~ "ninformational, social, presentational" 
  )) %>%
  mutate(param = round(param, digits=2)) %>%
  spread(parameter, param)

colnames(other_tab) <- c("Model", "Cost of negation", "Speaker optimality")

apa_table(other_tab, escape=FALSE, caption = "Inferred negation cost and speaker optimality parameters for all model variants.")
```

Other than speaker goal mixture weights explained in the main text (shown in Table \@ref(tab:phi)), the full model has two global parameters: the speaker's soft-max parameter $\lambda_{S_2}$ and soft-max paramater of the hypothetical speaker that the pragmatic listener reasons about $\lambda_{S_1}$.
$\lambda_{S_1}$ was 1, and $\lambda_{S_2}$ was inferred from the data: 
We put a prior that was consistent with those used for similar models in this model class: $\lambda_{S_2}$ ~ $Uniform(0,20)$.
Finally, we incorporate the literal semantics data into the RSA model by maintaining uncertainty about the semantic weight of utterance $w$ for state $s$, for each of the states and utterances, and assuming a Beta-Binomial linking function between these weights and the literal semantics data (see *Literal semantics task* above).
We infer the posterior distribution over all of the model parameters and generate model predictions based on this posterior distribution using Bayesian data analysis [@lee2014]. 
We ran 4 MCMC chains for 80,000 iterations, discarding the first 40,000 for burnin. 
The inferred values of parameters are shown in Table \@ref(tab:otherParams).

## Data Availability

Our model, preregistration of hypotheses, procedure, data, and analyses are available at \url{https://github.com/ejyoon/polite_speaker}. 

## Supplemental Figures

```{r utterance, echo=FALSE, out.width = "\\textwidth", fig.pos = "!h", fig.height=6, fig.cap="Experimental results (solid lines) and fitted predictions from the full model (dashed lines) for speaker production. Proportion of utterances chosen (utterance type – direct vs. indirect – in different colors and words shown on x-axis) given the true states (columns) and speaker goals (rows). Error bars represent 95\\% confidence intervals for the data and 95\\% highest density intervals for the model. Black dotted line represents the chance level."}
plot.utt <- ggplot(data=ms_utt %>%
                     filter(source == "data" | model == "full") %>%
  mutate(positivity = fct_recode(positivity, 
                               "It was ~" = "yes",
                               "It wasn't ~" = "not",
                               "It was ~" = "It was~",
                               "It wasn't ~" = "It wasn't~"
                               ),
         positivity = fct_relevel(positivity, "It was ~")) %>%
                     mutate(
                       # positivity = fct_relevel(positivity, "not"),
                            true_state = fct_recode(true_state,
                                                    "0 heart" = "0", 
                                                    "1 heart" = "1", 
                                                    "2 hearts" = "2", 
                                                    "3 hearts" = "3" 
                                                    ),
                            goal = fct_recode(goal, "kind" = "social")), 
       aes(x=utterance, y=prob, group = interaction(positivity, source), colour = positivity, linetype = source)) +
  geom_line()+
  facet_grid(goal~true_state, labeller = labeller(goal = label_both)) +
  xlab("utterance") +
  ylab("proportion chosen") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position="dodge") +
  geom_hline(yintercept=.1, lty=2) +
  ylim(0,1)+
  scale_color_ptol()+
  # scale_color_solarized(labels = c("It wasn't~","It was~"))+
  ggthemes::theme_few()+
  theme(axis.text.x = element_text(angle = 45, vjust=0.5),
        legend.position = "bottom") +
  guides(colour=guide_legend(title="utterance type")) +
  scale_linetype_discrete(labels = c("data", "model"))

plot.utt

ggsave("speaker_production_utt_wMod.png", plot = plot.utt, width = 7, height = 5,
       path = here::here("02_analysis/03_figs"))
```

```{r comparisonAll, echo=FALSE, out.width = "\\textwidth", fig.pos = "!h", fig.width=11, fig.height=7, fig.cap="Comparison of predictions for proportion of utterances chosen by pragmatic speaker from possible model variants (left) and human data (rightmost) for average proportion of negation produced among all utterances, given true state of 0 heart and speaker with a goal to be informative (top), kind (middle), or both (bottom). Gray dotted line indicates chance level at 12.5\\%."}
plot.comp.all <- ms_utt %>%  
  mutate(model = case_when(
    model == "NA" ~ "Human data", 
    model == "inf" ~ "model: \ninformational \nonly", 
    model == "soc" ~ "model: \nsocial only", 
    model == "pres" ~ "model: \npresentational \nonly", 
    model == "inf_pres" ~ "model: \ninformational, \npresentational", 
    model == "inf_soc" ~ "model: \ninformational, \nsocial", 
    model == "soc_pres" ~ "model: \nsocial, \npresentational", 
    model == "full" ~ "model: \ninformational, \nsocial, \npresentational" 
  ),
  model = fct_relevel(model, "model: \ninformational \nonly", "model: \nsocial only", "model: \npresentational \nonly", "model: \nsocial, \npresentational", "model: \ninformational, \nsocial", "model: \ninformational, \npresentational", "model: \ninformational, \nsocial, \npresentational")) %>%
  mutate(positivity = fct_recode(positivity, 
                               "It was ~" = "yes",
                               "It wasn't ~" = "not",
                               "It was ~" = "It was~",
                               "It wasn't ~" = "It wasn't~"
                               ),
         positivity = fct_relevel(positivity, "It was ~")) %>%
  mutate(
    # positivity = fct_recode(positivity,
    #                              "It was ~" = "yes",
    #                              "It wasn't ~" = "not"),
         # positivity = fct_relevel(positivity, "It wasn't ~"),
         goal = fct_recode(goal, "kind" = "social") 
         ) %>%
  filter(true_state == "0") %>%
  ggplot(., 
       aes(x=utterance, y=prob, fill=positivity, 
           # group = interaction(positive, source), linetype = forcats::fct_rev(positive),
           group = positivity,
           colour = positivity)) +
  geom_hline(yintercept=.125, lty=2, color="gray") +
  geom_line()+
  facet_grid(goal~model, labeller = labeller(goal = label_both)) +
  xlab("utterance") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position="dodge") +
  ylim(0,.7)+
  scale_color_ptol(guide=FALSE)+
  # scale_color_solarized(guide=FALSE)+
  ggthemes::theme_few(base_size = 15)+
  ylab("proportion chosen") +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),
        legend.position="bottom") +
  guides(color=guide_legend(title=NULL))

plot.comp.all

ggsave("model_comparisons_all.png", plot = plot.comp.all, width = 11, height = 4,
       path = here::here("02_analysis/03_figs"))

```

```{r negation, echo=FALSE, fig.width=11, fig.height=4, out.width = "\\textwidth", fig.pos = "!h", fig.cap="Experimental results (left) and fitted model predictions (right) for average proportion of negation produced among all utterances, given true states (x-axis) and goals (colors)."}
plot.neg <- ms_neg %>%  
  mutate(model = case_when(
    model == "NA" ~ "Human data", 
    model == "inf" ~ "model: \ninformational \nonly", 
    model == "soc" ~ "model: \nsocial only", 
    model == "pres" ~ "model: \npresentational \nonly", 
    model == "inf_pres" ~ "model: \ninformational, \npresentational", 
    model == "inf_soc" ~ "model: \ninformational, \nsocial", 
    model == "soc_pres" ~ "model: \nsocial, \npresentational", 
    model == "full" ~ "model: \ninformational, \nsocial, \npresentational" 
  ),
  model = fct_relevel(model, "model: \ninformational \nonly", "model: \nsocial only", "model: \npresentational \nonly", "model: \nsocial, \npresentational", "model: \ninformational, \nsocial", "model: \ninformational, \npresentational", "model: \ninformational, \nsocial, \npresentational")) %>%
  ggplot(., 
       aes(x=true_state, y=prob, color = goal, group=goal)) +
  geom_line(stat="identity", position=position_dodge()) +
  xlab("true state") +
  ylab("proportion negation") +
  geom_linerange(aes(ymin=ci_lower,ymax=ci_upper), position=position_dodge(width=.05)) +
  theme_few(base_size = 15)+
  scale_color_solarized() +
  facet_grid(.~model) +
  theme(legend.position="bottom")

plot.neg

ggsave("speaker_production_neg_wMod.png", plot = plot.neg, width = 7, height = 3,
       path = here::here("02_analysis/03_figs"))
```

\newpage

# References
```{r create_r-references}
r_refs(file = "politeness.bib")
```

\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

