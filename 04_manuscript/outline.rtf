{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww13840\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
Although \'93close the window!\'94 would be sufficient, \
we say \'93can you please \'85 ?\'94 or \'93would you mind \'85 ?\'94 \
And rather than telling the uncomfortable truth, \
we lie (\'93Your dress looks great\'94) and prevaricate (\'93your poem was so... appropriate to the occasion\'94).\
Utterances like these are a puzzle for standard views of language use, \
which see communication as the transfer of information from a sender to a receiver [@buhler, @jakobsen, @shannon, @frank2012]. \
On such views, speakers are informative and efficient, \
choosing utterances cooperatively so the listener can infer an intended meaning [@grice, @searle]. \
Polite speech \'96 like the examples above \'96 violates basic expectations about the nature of communication: \
it is typically underinformative and inefficient, and sometimes even outright false. \
So why are we polite? \
\
Theories of politeness explain deviations from optimal information transfer \
by assuming that speakers take into account social, as well as informational, concerns. \
These concerns are sometimes expressed as sets of polite maxims [@leech], or social norms [@watts], \
but the most influential account of politeness relies on the notion of \'93face\'94 to motivate deviations [@goffman1967,@brown]. \
On this theory, speakers seek to maintain freedom from imposition (\'93negative face\'94) \
as well as their positive self-image (\'93positive face\'94). \
Both inefficiencies and untruths in communication are then \
due to speakers\'92 strategic choices relative to possible face threats. \
\
This framework provides an intuitive and appealing explanation of many types of polite speech, \
but applying it to make quantitative predictions in any individual circumstance can be complicated. \
It is often not obvious what constitutes a face threat, \
or how a particular speech act should be adjusted to deal with that threat. \
Concretely, such theorizing does not constrain \
how an artificial agent like a robot should go about make polite requests, give negative evaluations, or deliver bad news. \
Further, it does not take into account the recursive nature of reasoning about face: \
speakers may choose particular strategies not just in order to preserve their listener\'92s or their own face, \
but also to be seen as doing so. \
\
To address these challenge, we develop a utility-theoretic model for understanding polite speech. \
In our model, speakers attempt to maximize a set of competing utilities: \
an informational utility, derived via effective information transmission; \
a social utility, derived by providing value to others; \
and a self-presentational utility, derived by appearing in a particular way to other agents. \
Speakers then can choose between different utterances on the basis of the utility they provide \
(as well as their cost to utter, approximated by the length of the utterance). \
The lie that a dress looks great provides social utility by making its wearer feel good, \
but does not inform about the true state of the world. \
Further, if the wearer suspects that the dress is in fact hideous, \
the speaker runs the risk of being seen as uninformative or even untruthful. \
\
Formally, these utilities are weighed within a rational speech act (RSA) model. \
RSA models take a probabilistic approach to understanding pragmatic reasoning in language. [@frank2012,@goodman2016]: \
speakers are modeled as agents who choose utterances \
by reasoning recursively about their effects on a listener relative to their cost; \
while listeners are modeled as choosing interpretations by reasoning about speakers. \
This class of models has been effective in understanding a wide variety of complex linguistic behaviors, \
including vagueness [@lassiter2015], hyperbole [@kao2014], and irony [@kao2015], among others. \
More broadly, RSA models provide a linguistic instantiation of the hypothesis \
that human social cognition can be approximated via reasoning about others as rational agents \
who act to maximize their subjective utility [@jaraettinger2016], \
a hypothesis which has found support in a wide variety of work with both adults and children [e.g., @baker2009, @liu2017].\
\
RSA models are defined recursively such that speakers reason about listeners, and vice versa. \
By convention the level of this recursion is numbered such that \
a pragmatic listener (L1) reasons about what intended meaning would have led a speaker (S1) to produce a particular utterance. \
Then S1 reasons about a \'93literal listener\'94 (L0), \
who is modeled as attending only to the literal meanings of words (rather than their pragmatic implications), \
and hence grounds the recursion. \
The target of our current work is a model of a polite speaker S2. \
S2  reasons about what utterance to send to L1 by considering the set of utilities described above: \
namely, whether an utterance results in L1 gaining information, feeling positively, \
or judging S2 to be either informative or kind. \
\
We evaluate our model by predicting human behavioral data in situations where polite language use is expected. \
Imagine Bob gave a presentation and is ignorant of the quality of his presentation; \
he asks Ann how well he did. \
Ann (the pragmatic speaker S2) produces an utterance w \
based on the true state of the world s (i.e., the rating truly deserved by Bob\'92s presentation) \
and a set of goal weights phi, \
each of which determine how much she would like to prioritize a particular goal compared to other possible goal(s). \
The speaker\'92s goal is represented as a utility function, \
based on which she chooses utterances approximately optimally (as per speaker optimality parameter \uc0\u955 S2; 14):\
.\
\
What goals must the speaker consider to arrive at a polite utterance? We consider three utilities: informational, social, and presentational. The total utility of an utterance is the weighted combination of the three utilities minus the cost C(w):\
\
\
\
This formulation contains three utility terms, each with its own weight (\uc0\u966 inf, \u966 soc, \u966 pres respectively). \
The first is a standard informational utility (Uinf), which represents the speaker\'92s desire to be epistemically helpful. \
The informational utility captures the amount of information a literal listener (L0) would still not know about the world state after hearing the speaker\'92s utterance:\
.\
For aspects of the world with affective consequences for the listener (e.g., Bob and his presentation), \
we assume speakers produce utterances that make listeners feel like they are in a good state. \
Social utility (Usoc) is the expected subjective utility to the listener of the state inferred given the utterance. \
This utility captures the idea that people want to hear that they are in a good state of the world (e.g., that Bob\'92s presentation was good). \
We use a simple linear value function (V) to map states to subjective utility values: better ratings are more positively valued. \
.\
If listeners are aware that speakers have goals and try to infer what those goals are, \
speakers may choose utterances in order to convey that they had certain goals in mind. \
The third component, presentational utility (Upres), \
captures the extent to which the speaker wants to appear to the listener to have a particular goal in mind (e.g. to be nice). \
This utility is defined recursively, and is the novel contribution of our model: \
Speakers gain presentational utility by having listeners believe they are trying to be informative or nice. Formally,\
.\
The speaker considers the listener L1 who hears an utterance and tries to jointly infer \
both the speaker\'92s utilities and the true state of the world: \
\
Finally, utterances that are more complex incur a greater cost \'96 \
capturing the general pressure towards economy in speech. \
In our work, negative utterances (e.g., \'93not terrible\'94) are slightly more costly than their positive equivalents \
(inferred from data; see Supplemental Materials). \
\
Intuitively, when Bob\'92s performance is good, Ann\'92s utilities align to lead her to say something positive. \
By saying \'93[Your talk] was amazing,\'94 Ann is being both truthful and kind, and that is likely to be clear to Bob. \
But, if Bob\'92s presentation is poor, Ann is in a bind: \
She could be kind and say it was great, but she does so at the cost of conveying the wrong information to Bob. Worse yet, Bob could infer that she is \'93just being nice\'94 and discount her comment. Alternatively, she could say the truth (\'93It was bad\'94), but then Bob would think Ann didn\'92t care about him. What is a socially-aware speaker to do? Our model predicts that indirect speech \'96 like \'93not terrible\'94 \'96 helps navigate Ann\'92s dilemma. It conveys some true information while being being sufficiently open-ended to spare Bob\'92s feelings. Further, by incurring the slightly higher cost involved in producing another word, it provide a signal to Bob that Ann takes his feelings into account in her choice. \
\
\
Fig. 1. Diagram of the pRSA model: The pragmatic speaker observes the true state and determines her goal between three utilities (epistemic, social, and presentational), and produces an utterance.\
\
We made a direct, pre-registered test of our model by instantiating the example above in an online experiment (N=202). Participants read scenarios in which we provided information on the speaker\'92s (Ann\'92s, in our example) feelings toward some performance or product (true state), which were shown on a scale from zero to three hearts (e.g. one out of three hearts). We also manipulated the speaker\'92s goal across trials: to be informative and give accurate feedback; to be social and to make the listener feel good; or to be both informative and social at the same time. In a single trial, each scenario was followed by a question that asked for the most likely utterance by Ann. Participants selected one of eight possible utterances, by choosing between It was vs. It wasn\'92t and then among terrible, bad, good, and amazing. \
Our primary behavioral hypothesis was that speakers who found themselves describing bad states (e.g., Bob\'92s performance was bad) and who had as goals to be both informative and social would produce more indirect, negative utterances (\'93It wasn\'92t terrible\'94). These indirect speech acts serve to save the listener\'92s face while also conveying a vague estimate of the true state. This prediction was confirmed: a logistics mixed effects model predicting negation as a function of true state and goal yielded a significant interaction, such that a speaker with both informational and social goals produced more negation in worse states compared to a speaker with only the informational goal (beta = 1.22) and a speaker with only social goal (beta = 0.566). Rather than eschewing one of their goals to increase utility along a single dimension, participants chose utterances that jointly satisfied their conflicting goals by producing indirect, polite speech. \
\
Figure 2. Full distribution of human responses vs. fitted model predictions for pragmatic speaker production. Error bars represent 95% confidence intervals for the data (vertical) and 95% highest density intervals for the model (horizontal).\
					\
To connect these behavioral data more directly to our model, we next built a Bayesian data analytic model to integrate out the parameters of the RSA model (e.g., the condition-specific goal-weights for the speaker) and provide a principled way to incorporate judgments about the literal meanings of the utterances into our model\'92s predictions (Lee & Wagenmakers, 2014; see Supplementary Materials). Using an independent sample of N=51 participants, we measured how participants judged our possible utterances to apply to each of the levels on the heart scale (e.g., to what extent is \'93terrible\'94 true of 2 out of 3 hearts?). These measurements are used in the Bayesian data analysis to approximate the semantics of the words as interpreted by the literal listener agent L0 (see Supplementary Materials for literal semantic results; see our pre-registered model, hypothesis, and procedure at FIXME).\
Predictions from the full polite speaker model showed a strong fit to participants utterance choices (r2(96) = 0.97; Figure 2). We also compared the predictions of our model with subset models containing different subsets of the three utilities in the full model (Figure 4; see Supplemental Materials: Model Comparison). Both the variance explained and the likelihood (FIXME) were the highest for the current model (see Table 1 and Figure 3). The current model yielded the closest predictions to the human data, especially for the condition in which the speaker was described to have both goals to be informative and social. Thus, all three -- informative, social, and presentational -- utilities were required to fully explain participants\'92 predictions.\
 \
Figure 3: Comparison of predictions for proportion of utterances chosen by pragmatic speaker from possible model variants (left) and human data (rightmost) for average proportion of negation produced among all utterances, given true state of 0 heart (on a scale of 0 to 3) and speaker with both goals. Gray dotted line indicates chance level at 12.5%.\
\
\
Table 1. Comparison of variance explained and Bayes factor for each model variant.\
\
Politeness is a puzzle for purely informational accounts of language use. Incorporating social motivations can provide an explanatory framework, but such intuitions have been resistant to formalization or precise testing. To overcome this issue, we created a utility-theoretic model of language use that captured the interplay between competing informational, social, and presentational goals. A preregistered experimental test of the model confirmed its ability to capture human judgments, unlike comparison models that used only a subset of the full utility structure. \
To better measure choice behavior, pur experiment abstracted away from natural interactions in a number of ways. Real-life Anns will have access to a potentially infinite range of utterances to manage the same tradeoff (\'93It\'92s so hard to give a good presentation,\'94 \'93That joke you made on slide 2 was brilliant!\'94). Under our framework, each will have strengths and weaknesses relative to the speaker\'92s goals, though computation in an unbounded model presents technical challenges [@monroe2016]. Furthermore, while one-shot interactions allow control over contextual variables, our model could be extended to model speakers across repeated interactions [@smith2013,@kleinschmidt2015]. \
Managing listeners\'92 inferences is a fundamental task for a socially conscious speaker. Following @brown we hypothesize that cross-cultural differences in politeness are a product of different weightings within the same utility structure. Systematic measurements of these weights could be an approach to understanding the vast range of politeness practices found across languages. Further, politeness is only one of the ways that language use deviates from pure information transfer. When we flirt, insult, boast, and empathize, we balance information transmission with the goal to affect others\'92 feelings or present particular views of ourselves. A similar utility structure to the one we employed here could give insights into these behaviors as well. \
Linguistic interactions require more than merely following simple rules (say \'93please\'94, \'93thank you\'94) at the right moments; they require balancing competing concerns. This balancing act extends beyond language understanding, permeating all aspects of social behavior.  Indeed, the goal-oriented view of polite language we develop here connects polite language with a wide range of behaviors that can be modeled as utility-driven inference in a social context [@refs].\
In sum, this work takes a concrete step toward quantitative models of the nuances of human speech. And it moves us closer to courteous computation \'96 to computers that communicate with tact.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 a robot should go about make polite requests, give negative evaluations, or deliver bad news. Further, it does not take into account the recursive nature of reasoning about face: speakers may choose particular strategies not just in order to preserve their listener\'92s or their own face, but also to be seen as doing so. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 To address these challenge, we develop a utility-theoretic model for understanding polite speech. In our model, speakers attempt to maximize a set of competing utilities: an informational utility, derived via effective information transmission; a social utility, derived by providing value to others; and a self-presentational utility, derived by appearing in a particular way to other agents. Speakers then can choose between different utterances on the basis of the utility they provide (as well as their cost to utter, approximated by the length of the utterance). The lie that a dress looks great provides social utility by making its wearer feel good, but does not inform about the true state of the world. Further, if the wearer suspects that the dress is in fact hideous, the speaker runs the risk of being seen as uninformative or even untruthful. \
Formally, these utilities are weighed within a rational speech act (RSA) model. RSA models take a probabilistic approach to understanding pragmatic reasoning in language. [@frank2012,@goodman2016]: speakers are modeled as agents who choose utterances by reasoning recursively about their effects on a listener relative to their cost; while listeners are modeled as choosing interpretations by reasoning about speakers. This class of models has been effective in understanding a wide variety of complex linguistic behaviors, including vagueness [@lassiter2015], hyperbole [@kao2014], and irony [@kao2015], among others. More broadly, RSA models provide a linguistic instantiation of the hypothesis that human social cognition can be approximated via reasoning about others as rational agents who act to maximize their subjective utility [@jaraettinger2016], a hypothesis which has found support in a wide variety of work with both adults and children [e.g., @baker2009, @liu2017].\
RSA models are defined recursively such that speakers reason about listeners, and vice versa. By convention the level of this recursion is numbered such that a pragmatic listener (L1) reasons about what intended meaning would have led a speaker (S1) to produce a particular utterance. Then S1 reasons about a \'93literal listener\'94 (L0), who is modeled as attending only to the literal meanings of words (rather than their pragmatic implications), and hence grounds the recursion. The target of our current work is a model of a polite speaker S2. S2  reasons about what utterance to send to L1 by considering the set of utilities described above: namely, whether an utterance results in L1 gaining information, feeling positively, or judging S2 to be either informative or kind. \
We evaluate our model by predicting human behavioral data in situations where polite language use is expected. Imagine Bob gave a presentation and is ignorant of the quality of his presentation; he asks Ann how well he did. Ann (the pragmatic speaker S2) produces an utterance w based on the true state of the world s (i.e., the rating truly deserved by Bob\'92s presentation) and a set of goal weights phi, each of which determine how much she would like to prioritize a particular goal compared to other possible goal(s). The speaker\'92s goal is represented as a utility function, based on which she chooses utterances approximately optimally (as per speaker optimality parameter \uc0\u955 S2; 14):\
.\
\
What goals must the speaker consider to arrive at a polite utterance? We consider three utilities: informational, social, and presentational. The total utility of an utterance is the weighted combination of the three utilities minus the cost C(w):\
\
\
\
This formulation contains three utility terms, each with its own weight (\uc0\u966 inf, \u966 soc, \u966 pres respectively). The first is a standard informational utility (Uinf), which represents the speaker\'92s desire to be epistemically helpful. The informational utility captures the amount of information a literal listener (L0) would still not know about the world state after hearing the speaker\'92s utterance:\
.\
For aspects of the world with affective consequences for the listener (e.g., Bob and his presentation), we assume speakers produce utterances that make listeners feel like they are in a good state. Social utility (Usoc) is the expected subjective utility to the listener of the state inferred given the utterance. This utility captures the idea that people want to hear that they are in a good state of the world (e.g., that Bob\'92s presentation was good). We use a simple linear value function (V) to map states to subjective utility values: better ratings are more positively valued. \
.\
If listeners are aware that speakers have goals and try to infer what those goals are, speakers may choose utterances in order to convey that they had certain goals in mind. The third component, presentational utility (Upres), captures the extent to which the speaker wants to appear to the listener to have a particular goal in mind (e.g. to be nice). This utility is defined recursively, and is the novel contribution of our model: Speakers gain presentational utility by having listeners believe they are trying to be informative or nice. Formally,\
.\
The speaker considers the listener L1 who hears an utterance and tries to jointly infer both the speaker\'92s utilities and the true state of the world: \
\
Finally, utterances that are more complex incur a greater cost \'96 capturing the general pressure towards economy in speech. In our work, negative utterances (e.g., \'93not terrible\'94) are slightly more costly than their positive equivalents (inferred from data; see Supplemental Materials). \
Intuitively, when Bob\'92s performance is good, Ann\'92s utilities align to lead her to say something positive. By saying \'93[Your talk] was amazing,\'94 Ann is being both truthful and kind, and that is likely to be clear to Bob. But, if Bob\'92s presentation is poor, Ann is in a bind: She could be kind and say it was great, but she does so at the cost of conveying the wrong information to Bob. Worse yet, Bob could infer that she is \'93just being nice\'94 and discount her comment. Alternatively, she could say the truth (\'93It was bad\'94), but then Bob would think Ann didn\'92t care about him. What is a socially-aware speaker to do? Our model predicts that indirect speech \'96 like \'93not terrible\'94 \'96 helps navigate Ann\'92s dilemma. It conveys some true information while being being sufficiently open-ended to spare Bob\'92s feelings. Further, by incurring the slightly higher cost involved in producing another word, it provide a signal to Bob that Ann takes his feelings into account in her choice. \
\
\
Fig. 1. Diagram of the pRSA model: The pragmatic speaker observes the true state and determines her goal between three utilities (epistemic, social, and presentational), and produces an utterance.\
\
We made a direct, pre-registered test of our model by instantiating the example above in an online experiment (N=202). Participants read scenarios in which we provided information on the speaker\'92s (Ann\'92s, in our example) feelings toward some performance or product (true state), which were shown on a scale from zero to three hearts (e.g. one out of three hearts). We also manipulated the speaker\'92s goal across trials: to be informative and give accurate feedback; to be social and to make the listener feel good; or to be both informative and social at the same time. In a single trial, each scenario was followed by a question that asked for the most likely utterance by Ann. Participants selected one of eight possible utterances, by choosing between It was vs. It wasn\'92t and then among terrible, bad, good, and amazing. \
Our primary behavioral hypothesis was that speakers who found themselves describing bad states (e.g., Bob\'92s performance was bad) and who had as goals to be both informative and social would produce more indirect, negative utterances (\'93It wasn\'92t terrible\'94). These indirect speech acts serve to save the listener\'92s face while also conveying a vague estimate of the true state. This prediction was confirmed: a logistics mixed effects model predicting negation as a function of true state and goal yielded a significant interaction, such that a speaker with both informational and social goals produced more negation in worse states compared to a speaker with only the informational goal (beta = 1.22) and a speaker with only social goal (beta = 0.566). Rather than eschewing one of their goals to increase utility along a single dimension, participants chose utterances that jointly satisfied their conflicting goals by producing indirect, polite speech. \
\
Figure 2. Full distribution of human responses vs. fitted model predictions for pragmatic speaker production. Error bars represent 95% confidence intervals for the data (vertical) and 95% highest density intervals for the model (horizontal).\
					\
To connect these behavioral data more directly to our model, we next built a Bayesian data analytic model to integrate out the parameters of the RSA model (e.g., the condition-specific goal-weights for the speaker) and provide a principled way to incorporate judgments about the literal meanings of the utterances into our model\'92s predictions (Lee & Wagenmakers, 2014; see Supplementary Materials). Using an independent sample of N=51 participants, we measured how participants judged our possible utterances to apply to each of the levels on the heart scale (e.g., to what extent is \'93terrible\'94 true of 2 out of 3 hearts?). These measurements are used in the Bayesian data analysis to approximate the semantics of the words as interpreted by the literal listener agent L0 (see Supplementary Materials for literal semantic results; see our pre-registered model, hypothesis, and procedure at FIXME).\
Predictions from the full polite speaker model showed a strong fit to participants utterance choices (r2(96) = 0.97; Figure 2). We also compared the predictions of our model with subset models containing different subsets of the three utilities in the full model (Figure 4; see Supplemental Materials: Model Comparison). Both the variance explained and the likelihood (FIXME) were the highest for the current model (see Table 1 and Figure 3). The current model yielded the closest predictions to the human data, especially for the condition in which the speaker was described to have both goals to be informative and social. Thus, all three -- informative, social, and presentational -- utilities were required to fully explain participants\'92 predictions.\
 \
Figure 3: Comparison of predictions for proportion of utterances chosen by pragmatic speaker from possible model variants (left) and human data (rightmost) for average proportion of negation produced among all utterances, given true state of 0 heart (on a scale of 0 to 3) and speaker with both goals. Gray dotted line indicates chance level at 12.5%.\
\
\
Table 1. Comparison of variance explained and Bayes factor for each model variant.\
\
Politeness is a puzzle for purely informational accounts of language use. Incorporating social motivations can provide an explanatory framework, but such intuitions have been resistant to formalization or precise testing. To overcome this issue, we created a utility-theoretic model of language use that captured the interplay between competing informational, social, and presentational goals. A preregistered experimental test of the model confirmed its ability to capture human judgments, unlike comparison models that used only a subset of the full utility structure. \
To better measure choice behavior, pur experiment abstracted away from natural interactions in a number of ways. Real-life Anns will have access to a potentially infinite range of utterances to manage the same tradeoff (\'93It\'92s so hard to give a good presentation,\'94 \'93That joke you made on slide 2 was brilliant!\'94). Under our framework, each will have strengths and weaknesses relative to the speaker\'92s goals, though computation in an unbounded model presents technical challenges [@monroe2016]. Furthermore, while one-shot interactions allow control over contextual variables, our model could be extended to model speakers across repeated interactions [@smith2013,@kleinschmidt2015]. \
Managing listeners\'92 inferences is a fundamental task for a socially conscious speaker. Following @brown we hypothesize that cross-cultural differences in politeness are a product of different weightings within the same utility structure. Systematic measurements of these weights could be an approach to understanding the vast range of politeness practices found across languages. Further, politeness is only one of the ways that language use deviates from pure information transfer. When we flirt, insult, boast, and empathize, we balance information transmission with the goal to affect others\'92 feelings or present particular views of ourselves. A similar utility structure to the one we employed here could give insights into these behaviors as well. \
Linguistic interactions require more than merely following simple rules (say \'93please\'94, \'93thank you\'94) at the right moments; they require balancing competing concerns. This balancing act extends beyond language understanding, permeating all aspects of social behavior.  Indeed, the goal-oriented view of polite language we develop here connects polite language with a wide range of behaviors that can be modeled as utility-driven inference in a social context [@refs].\
In sum, this work takes a concrete step toward quantitative models of the nuances of human speech. And it moves us closer to courteous computation \'96 to computers that communicate with tact.\
}